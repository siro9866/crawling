# ê²½ë¡œì— í¬í•¨ëœ ëª¨ë“  resource íŒŒì¼ ë‹¤ìš´ë¡œë“œ
import asyncio
import os
import requests
from urllib.parse import urljoin, urlparse, unquote
from bs4 import BeautifulSoup
from playwright.async_api import async_playwright

# í™•ì¥ì ê¸°ë°˜ í´ë” ë§¤í•‘
EXTENSION_DIR_MAP = {
    ".jpg": "img", ".jpeg": "img", ".png": "img", ".gif": "img", ".webp": "img", ".svg": "img", ".ico": "img",
    ".css": "css",
    ".js": "js",
    ".woff": "font", ".woff2": "font", ".ttf": "font", ".otf": "font",
}

# ì €ì¥ ê²½ë¡œ ì„¤ì •
def get_save_path(base_url, url):
    baseParsed = urlparse(base_url)
    parsed = urlparse(url)
    ext = os.path.splitext(parsed.path)[-1].lower()
    subdir = EXTENSION_DIR_MAP.get(ext, "other")
    os.makedirs(os.path.join(baseParsed.hostname, subdir), exist_ok=True)
    filename = unquote(os.path.basename(parsed.path)) or "index.html"
    return os.path.join(baseParsed.hostname, subdir, filename)

# ë‹¤ìš´ë¡œë“œ í•¨ìˆ˜
def download_file(base_url, url):
    try:
        path = get_save_path(base_url, url)
        if not os.path.exists(path):
            headers = {"User-Agent": "Mozilla/5.0"}
            r = requests.get(url, headers=headers, timeout=10)
            r.raise_for_status()
            with open(path, "wb") as f:
                f.write(r.content)
            print(f"âœ… ì €ì¥ ì™„ë£Œ: {path}")
    except Exception as e:
        print(f"âŒ ì‹¤íŒ¨: {url} ({e})")

# ë©”ì¸ ì‹¤í–‰
async def get_resources(base_url):
    parsed = urlparse(base_url)
    os.makedirs(parsed.hostname, exist_ok=True)

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()

        print("ğŸ”— í˜ì´ì§€ ì ‘ì† ì¤‘...")
        await page.goto(base_url, wait_until="networkidle")

        # ìŠ¤í¬ë¡¤ë¡œ lazy-load ìœ ë„
        for _ in range(10):
            await page.mouse.wheel(0, 1000)
            await asyncio.sleep(1)

        content = await page.content()
        soup = BeautifulSoup(content, "html.parser")

        resource_urls = set()

        # ì´ë¯¸ì§€
        for img in soup.find_all("img"):
            src = img.get("src")
            if src:
                resource_urls.add(urljoin(base_url, src))

        # CSS, favicon, preload ë“±
        for link in soup.find_all("link", href=True):
            rel = link.get("rel", [])
            if any(r in rel for r in ["stylesheet", "icon", "preload", "apple-touch-icon"]):
                resource_urls.add(urljoin(base_url, link["href"]))

        # JS
        for script in soup.find_all("script", src=True):
            resource_urls.add(urljoin(base_url, script["src"]))

        print(f"ğŸ“¦ ì´ ë¦¬ì†ŒìŠ¤ ìˆ˜: {len(resource_urls)}ê°œ")

        # ë‹¤ìš´ë¡œë“œ ì‹¤í–‰
        for url in resource_urls:
            download_file(base_url, url)

        await browser.close()
        print("ğŸ‰ ëª¨ë“  ë¦¬ì†ŒìŠ¤ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ.")

# ì‹¤í–‰
BASE_URL = "https://news.naver.com"
asyncio.run(get_resources(BASE_URL))