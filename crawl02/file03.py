# ê²½ë¡œì— í¬í•¨ëœ ëª¨ë“  resource íŒŒì¼ ë‹¤ìš´ë¡œë“œ
import asyncio
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
import os
import requests
from urllib.parse import urljoin, urlparse, unquote

SAVE_DIR = "naver_news_img"

# ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(SAVE_DIR, exist_ok=True)

def sanitize_filename(url):
    parsed = urlparse(url)
    filename = os.path.basename(parsed.path)
    return unquote(filename) if filename else "default.jpg"

def download_image(url):
    try:
        filename = sanitize_filename(url)
        path = os.path.join(SAVE_DIR, filename)

        if not os.path.exists(path):
            headers = {"User-Agent": "Mozilla/5.0"}
            r = requests.get(url, headers=headers, timeout=10)
            r.raise_for_status()
            with open(path, "wb") as f:
                f.write(r.content)
            print(f"âœ… ì €ì¥ ì™„ë£Œ: {filename}")
    except Exception as e:
        print(f"âŒ ì‹¤íŒ¨: {url} ({e})")

async def main():
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False)
        context = await browser.new_context()
        page = await context.new_page()

        print("ğŸ§­ í˜ì´ì§€ ë¡œë”© ì¤‘...")
        await page.goto("https://news.naver.com", wait_until="networkidle")

        # ìŠ¤í¬ë¡¤ ë‹¤ìš´í•˜ì—¬ lazy load ìœ ë„
        for _ in range(10):
            await page.mouse.wheel(0, 1000)
            await asyncio.sleep(1)

        # HTML ê°€ì ¸ì˜¤ê¸°
        content = await page.content()

        # BeautifulSoup ìœ¼ë¡œ ì´ë¯¸ì§€ íŒŒì‹±
        soup = BeautifulSoup(content, "html.parser")
        img_tags = soup.find_all("img")

        print(f"ğŸ“¸ ì´ë¯¸ì§€ íƒœê·¸ ìˆ˜: {len(img_tags)}")

        img_urls = set()
        for img in img_tags:
            src = img.get("src")
            if src:
                full_url = urljoin("https://news.naver.com", src)
                img_urls.add(full_url)

        print(f"ğŸ–¼ï¸ ìœ íš¨ ì´ë¯¸ì§€ URL ìˆ˜: {len(img_urls)}")

        # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
        for url in img_urls:
            download_image(url)

        await browser.close()
        print("âœ… ëª¨ë“  ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ.")

asyncio.run(main())